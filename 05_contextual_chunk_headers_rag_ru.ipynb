{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Контекстные заголовки чанков (CCH) в Simple RAG\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) повышает фактическую точность языковых моделей, извлекая релевантные внешние знания перед генерацией ответа. Однако стандартное разбиение на чанки часто теряет важный контекст, снижая эффективность поиска.\n",
    "\n",
    "Contextual Chunk Headers (CCH) улучшают RAG, добавляя высокоуровневый контекст (например, заголовки документов или разделов) к каждому чанку перед их векторизацией. Это повышает качество поиска и предотвращает ответы вне контекста.\n",
    "\n",
    "## Шаги в этом ноутбуке:\n",
    "\n",
    "1. **Загрузка данных**: Загрузка и предварительная обработка текстовых данных.\n",
    "2. **Разбиение с контекстными заголовками**: Извлечение заголовков разделов и добавление их к чанкам.\n",
    "3. **Создание эмбеддингов**: Преобразование чанков с контекстом в числовые представления.\n",
    "4. **Семантический поиск**: Поиск релевантных чанков по пользовательскому запросу.\n",
    "5. **Генерация ответа**: Использование языковой модели для генерации ответа из найденного текста.\n",
    "6. **Оценка**: Анализ точности ответов с помощью системы оценки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройка окружения\n",
    "Начнём с импорта необходимых библиотек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import fitz\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Извлечение текста и определение заголовков разделов\n",
    "Извлекаем текст из PDF, одновременно определяя заголовки разделов (потенциальные заголовки для чанков)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Извлекает текст из PDF файла и выводит первые `num_chars` символов.\n",
    "\n",
    "    Аргументы:\n",
    "    pdf_path (str): Путь к PDF файлу.\n",
    "\n",
    "    Возвращает:\n",
    "    str: Извлечённый текст из PDF.\n",
    "    \"\"\"\n",
    "    # Открываем PDF файл\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # Инициализируем пустую строку для хранения текста\n",
    "\n",
    "    # Итерируемся по страницам PDF\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # Получаем страницу\n",
    "        text = page.get_text(\"text\")  # Извлекаем текст со страницы\n",
    "        all_text += text  # Добавляем текст к общей строке\n",
    "\n",
    "    return all_text  # Возвращаем извлечённый текст"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройка клиента OpenAI API\n",
    "Инициализируем клиент OpenAI для генерации эмбеддингов и ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем клиент OpenAI с базовым URL и API ключом\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # Получаем API ключ из переменных окружения\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбиение текста с контекстными заголовками\n",
    "Для улучшения поиска генерируем описательные заголовки для каждого чанка с помощью LLM модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chunk_header(chunk, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    Генерирует заголовок для заданного текстового чанка с помощью LLM.\n",
    "\n",
    "    Аргументы:\n",
    "    chunk (str): Текстовый чанк для генерации заголовка.\n",
    "    model (str): Модель для генерации заголовка. По умолчанию \"meta-llama/Llama-3.2-3B-Instruct\".\n",
    "\n",
    "    Возвращает:\n",
    "    str: Сгенерированный заголовок.\n",
    "    \"\"\"\n",
    "    # Определяем системный промт для управления поведением ИИ\n",
    "    system_prompt = \"Generate a concise and informative title for the given text.\"\n",
    "    \n",
    "    # Генерируем ответ от ИИ модели на основе системного промта и текстового чанка\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": chunk}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Возвращаем сгенерированный заголовок, удаляя лишние пробелы\n",
    "    return response.choices[0].message.content.strip()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}