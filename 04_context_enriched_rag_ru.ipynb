{
 "cells": [

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация ответа с расширенным контекстом\n",
    "Используем обогащённый контекст для генерации более точных ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_enriched_response(query, context, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    Генерирует ответ на основе расширенного контекста.\n",
    "    \n",
    "    Аргументы:\n",
    "    query (str): Пользовательский запрос.\n",
    "    context (str): Расширенный контекст.\n",
    "    model (str): Модель для генерации ответа.\n",
    "    \n",
    "    Возвращает:\n",
    "    str: Сгенерированный ответ.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Ты - AI ассистент, который строго отвечает на основе предоставленного контекста. Если ответа нет в контексте, скажи 'Недостаточно информации для ответа'.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Контекст:\\n{context}\\n\\nВопрос: {query}\"}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка качества ответов\n",
    "Сравниваем качество ответов с обычным и обогащённым контекстом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response(question, response, true_answer):\n",
    "    \"\"\"\n",
    "    Оценивает качество ответа по двум критериям: достоверность и релевантность.\n",
    "    \n",
    "    Аргументы:\n",
    "    question (str): Оригинальный вопрос.\n",
    "    response (str): Оцениваемый ответ.\n",
    "    true_answer (str): Эталонный ответ.\n",
    "    \n",
    "    Возвращает:\n",
    "    dict: Оценки по шкале 0-1 для каждого критерия.\n",
    "    \"\"\"\n",
    "    # Промт для оценки достоверности\n",
    "    faithfulness_prompt = f\"\"\"\n",
    "    Оцените достоверность ответа по сравнению с эталоном:\n",
    "    Вопрос: {question}\n",
    "    Ответ: {response}\n",
    "    Эталон: {true_answer}\n",
    "    \n",
    "    Верните только число от 0 до 1, где:\n",
    "    1 - полностью соответствует эталону\n",
    "    0.5 - частично соответствует\n",
    "    0 - не соответствует\n",
    "    \"\"\"\n",
    "    \n",
    "    # Промт для оценки релевантности\n",
    "    relevancy_prompt = f\"\"\"\n",
    "    Оцените релевантность ответа вопросу:\n",
    "    Вопрос: {question}\n",
    "    Ответ: {response}\n",
    "    \n",
    "    Верните только число от 0 до 1, где:\n",
    "    1 - полностью отвечает на вопрос\n",
    "    0.5 - частично отвечает\n",
    "    0 - не отвечает\n",
    "    \"\"\"\n",
    "    \n",
    "    # Получаем оценки\n",
    "    faithfulness = float(client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[{\"role\": \"user\", \"content\": faithfulness_prompt}],\n",
    "        temperature=0\n",
    "    ).choices[0].message.content)\n",
    "    \n",
    "    relevancy = float(client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[{\"role\": \"user\", \"content\": relevancy_prompt}],\n",
    "        temperature=0\n",
    "    ).choices[0].message.content)\n",
    "    \n",
    "    return {\"faithfulness\": faithfulness, \"relevancy\": relevancy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример полного цикла работы\n",
    "Демонстрация работы контекстно-обогащённого RAG на реальном запросе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_pipeline(query, documents, document_embeddings):\n",
    "    \"\"\"\n",
    "    Выполняет полный цикл контекстно-обогащённого RAG.\n",
    "    \n",
    "    Аргументы:\n",
    "    query (str): Пользовательский запрос.\n",
    "    documents (List[dict]): Список документов.\n",
    "    document_embeddings (List[np.ndarray]): Эмбеддинги документов.\n",
    "    \n",
    "    Возвращает:\n",
    "    dict: Результаты с базовым и улучшенным ответами.\n",
    "    \"\"\"\n",
    "    # 1. Находим релевантные документы\n",
    "    relevant_docs = semantic_search(query, documents, document_embeddings)\n",
    "    \n",
    "    # 2. Извлекаем фрагменты из топ-1 документа\n",
    "    chunks = extract_relevant_chunks(relevant_docs[0]['document'], query)\n",
    "    \n",
    "    # 3. Генерируем базовый ответ (без расширения контекста)\n",
    "    basic_context = \"\\n\".join([chunk['text'] for chunk in chunks])\n",
    "    basic_response = generate_enriched_response(query, basic_context)\n",
    "    \n",
    "    # 4. Расширяем контекст\n",
    "    enriched_context = expand_context(chunks, query)\n",
    "    enriched_response = generate_enriched_response(query, enriched_context)\n",
    "    \n",
    "    return {\n",
    "        \"basic_response\": basic_response,\n",
    "        \"enriched_response\": enriched_response,\n",
    "        \"context_size\": {\n",
    "            \"basic\": len(basic_context.split()),\n",
    "            \"enriched\": len(enriched_context.split())\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запуск и оценка\n",
    "Загружаем данные и запускаем полный цикл обработки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем документы\n",
    "documents = load_documents(\"data\")\n",
    "\n",
    "# Создаём эмбеддинги\n",
    "document_embeddings = create_document_embeddings(documents)\n",
    "\n",
    "# Загружаем тестовые вопросы\n",
    "with open('data/val.json') as f:\n",
    "    test_questions = json.load(f)\n",
    "\n",
    "# Выбираем первый вопрос для демонстрации\n",
    "query = test_questions[0]['question']\n",
    "true_answer = test_questions[0]['ideal_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запускаем полный цикл\n",
    "results = run_full_pipeline(query, documents, document_embeddings)\n",
    "\n",
    "# Оцениваем результаты\n",
    "basic_evaluation = evaluate_response(query, results[\"basic_response\"], true_answer)\n",
    "enriched_evaluation = evaluate_response(query, results[\"enriched_response\"], true_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод результатов\n",
    "Сравниваем качество ответов с разными подходами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Базовый ответ ({results['context_size']['basic']} слов):\\n{results['basic_response']}\\n\")\n",
    "print(f\"Оценка: Достоверность={basic_evaluation['faithfulness']}, Релевантность={basic_evaluation['relevancy']}\\n\")\n",
    "\n",
    "print(f\"Обогащённый ответ ({results['context_size']['enriched']} слов):\\n{results['enriched_response']}\\n\")\n",
    "print(f\"Оценка: Достоверность={enriched_evaluation['faithfulness']}, Релевантность={enriched_evaluation['relevancy']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}