{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## Оценка размеров фрагментов в простом RAG\n",
    "\n",
    "Выбор оптимального размера фрагментов текста критически важен для эффективности поиска в конвейере Retrieval-Augmented Generation (RAG). Наша цель - найти баланс между:\n",
    "- Точностью поиска (релевантность найденных фрагментов)\n",
    "- Полнотой информации (достаточный контекст для генерации ответа)\n",
    "\n",
    "Методика оценки включает следующие этапы:\n",
    "\n",
    "1. Извлечение текста из PDF-документа\n",
    "2. Сегментация текста на фрагменты разной длины\n",
    "3. Векторизация (создание эмбеддингов) для каждого фрагмента\n",
    "4. Семантический поиск по векторному пространству\n",
    "5. Генерация ответа на основе найденных фрагментов\n",
    "6. Оценка качества ответов по двум метрикам:\n",
    "   - Достоверность (соответствие исходным данным)\n",
    "   - Релевантность (соответствие вопросу)\n",
    "7. Сравнительный анализ для разных размеров фрагментов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройка рабочего окружения\n",
    "Импортируем необходимые Python-библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Конфигурация клиента OpenAI API\n",
    "Инициализация подключения к API для:\n",
    "- Генерации векторных представлений текста (эмбеддингов)\n",
    "- Формирования итоговых ответов на вопросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем клиент OpenAI с базовым URL и API ключом\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # Получаем API ключ из переменных окружения\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Экстракция текста из PDF\n",
    "Используем библиотеку PyMuPDF (fitz) для извлечения текста из документа `AI_Information.pdf`.\n",
    "Особенности обработки:\n",
    "- Сохранение структуры абзацев\n",
    "- Удаление лишних пробелов\n",
    "- Конкатенация текста со всех страниц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intellectual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the past f\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Извлекает текстовое содержимое из PDF документа.\n",
    "\n",
    "    Параметры:\n",
    "    pdf_path (str): Путь к PDF файлу\n",
    "\n",
    "    Возвращаемое значение:\n",
    "    str: Текст документа, объединённый со всех страниц\n",
    "    \"\"\"\n",
    "    # Открываем PDF-файл\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # Инициализируем пустую строку для хранения извлечённого текста\n",
    "    \n",
    "    # Итерируемся по каждой странице в PDF\n",
    "    for page in mypdf:\n",
    "        # Извлекаем текст с текущей страницы и добавляем пробел\n",
    "        all_text += page.get_text(\"text\") + \" \"\n",
    "\n",
    "    # Возвращаем извлечённый текст, очищенный от начальных/конечных пробелов\n",
    "    return all_text.strip()\n",
    "\n",
    "# Определяем путь к PDF-файлу\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# Извлекаем текст из PDF-файла\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Печатаем первые 500 символов извлечённого текста\n",
    "print(extracted_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сегментация текста на фрагменты\n",
    "Алгоритм разделения текста с перекрытием (overlap) обеспечивает:\n",
    "- Сохранение контекста между соседними фрагментами\n",
    "- Гибкость в выборе размера фрагментов\n",
    "- Возможность сравнения разных стратегий чанкинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Size: 128, Number of Chunks: 326\n",
      "Chunk Size: 256, Number of Chunks: 164\n",
      "Chunk Size: 512, Number of Chunks: 82\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    Сегментирует текст на фрагменты с заданным перекрытием.\n",
    "\n",
    "    Параметры:\n",
    "    text (str): Исходный текст для сегментации\n",
    "    n (int): Максимальная длина фрагмента в символах\n",
    "    overlap (int): Размер перекрытия между соседними фрагментами\n",
    "\n",
    "    Возвращаемое значение:\n",
    "    List[str]: Список текстовых фрагментов\n",
    "    \"\"\"\n",
    "    chunks = []  # Инициализируем пустой список для хранения чанков\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Добавляем чанк текста от текущего индекса до индекса + размер чанка\n",
    "        chunks.append(text[i:i + n])\n",
    "    \n",
    "    return chunks  # Возвращаем список текстовых чанков\n",
    "\n",
    "# Определяем различные размеры чанков для оценки\n",
    "chunk_sizes = [128, 256, 512]\n",
    "\n",
    "# Создаём словарь для хранения текстовых чанков для каждого размера\n",
    "text_chunks_dict = {size: chunk_text(extracted_text, size, size // 5) for size in chunk_sizes}\n",
    "\n",
    "# Печатаем количество созданных чанков для каждого размера\n",
    "for size, chunks in text_chunks_dict.items():\n",
    "    print(f\"Chunk Size: {size}, Number of Chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Векторизация текстовых фрагментов\n",
    "Преобразование текста в векторные представления (эмбеддинги) позволяет:\n",
    "- Сравнивать семантическое сходство текстов\n",
    "- Эффективно индексировать и искать информацию\n",
    "- Использовать математические операции над текстовыми данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|██████████| 3/3 [00:11<00:00,  3.71s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n",
    "    \"\"\"\n",
    "    Создаёт векторные представления для списка текстов.\n",
    "\n",
    "    Параметры:\n",
    "    texts (List[str]): Тексты для векторизации\n",
    "    model (str): Идентификатор модели эмбеддингов\n",
    "\n",
    "    Возвращаемое значение:\n",
    "    List[np.ndarray]: Список векторных представлений\n",
    "    \"\"\"\n",
    "    # Создаём эмбеддинги с использованием указанной модели\n",
    "    response = client.embeddings.create(model=model, input=texts)\n",
    "    # Преобразуем ответ в список numpy массивов и возвращаем\n",
    "    return [np.array(embedding.embedding) for embedding in response.data]\n",
    "\n",
    "# Генерируем эмбеддинги для каждого размера чанков\n",
    "# Итерируемся по каждому размеру чанков и соответствующим чанкам в text_chunks_dict\n",
    "chunk_embeddings_dict = {size: create_embeddings(chunks) for size, chunks in tqdm(text_chunks_dict.items(), desc=\"Generating Embeddings\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семантический поиск по векторному пространству\n",
    "Алгоритм поиска основан на:\n",
    "- Косинусной мере схожести векторов\n",
    "- Ранжировании результатов по релевантности\n",
    "- Возможности настройки количества возвращаемых фрагментов (top-k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Вычисляет косинусное сходство между векторами.\n",
    "\n",
    "    Параметры:\n",
    "    vec1 (np.ndarray): Первый вектор\n",
    "    vec2 (np.ndarray): Второй вектор\n",
    "\n",
    "    Возвращаемое значение:\n",
    "    float: Значение косинусного сходства [-1, 1]\n",
    "    \"\"\"\n",
    "\n",
    "    # Вычисляем скалярное произведение двух векторов\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query, text_chunks, chunk_embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Находит топ-k наиболее релевантных текстовых чанков.\n",
    "    \n",
    "    Аргументы:\n",
    "    query (str): Пользовательский запрос.\n",
    "    text_chunks (List[str]): Список текстовых чанков.\n",
    "    chunk_embeddings (List[np.ndarray]): Эмбеддинги текстовых чанков.\n",
    "    k (int): Количество возвращаемых топовых чанков.\n",
    "    \n",
    "    Возвращает:\n",
    "    List[str]: Наиболее релевантные текстовые чанки.\n",
    "    \"\"\"\n",
    "    # Генерируем эмбеддинг для запроса - передаём запрос как список и берём первый элемент\n",
    "    query_embedding = create_embeddings([query])[0]\n",
    "    \n",
    "    # Вычисляем косинусную схожесть между эмбеддингом запроса и каждым эмбеддингом чанка\n",
    "    similarities = [cosine_similarity(query_embedding, emb) for emb in chunk_embeddings]\n",
    "    \n",
    "    # Получаем индексы топ-k наиболее схожих чанков\n",
    "    top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    \n",
    "    # Возвращаем топ-k наиболее релевантных текстовых чанков\n",
    "    return [text_chunks[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AI enables personalized medicine by analyzing individual patient data, predicting treatment \\nresponses, and tailoring interventions. Personalized medicine enhances treatment effectiveness \\nand reduces adverse effects. \\nRobotic Surgery \\nAI-powered robotic s', ' analyzing biological data, predicting drug \\nefficacy, and identifying potential drug candidates. AI-powered systems reduce the time and cost \\nof bringing new treatments to market. \\nPersonalized Medicine \\nAI enables personalized medicine by analyzing indiv', 'g \\npatient outcomes, and assisting in treatment planning. AI-powered tools enhance accuracy, \\nefficiency, and patient care. \\nDrug Discovery and Development \\nAI accelerates drug discovery and development by analyzing biological data, predicting drug \\neffica', 'mains. \\nThese applications include: \\nHealthcare \\nAI is transforming healthcare through applications such as medical diagnosis, drug discovery, \\npersonalized medicine, and robotic surgery. AI-powered tools can analyze medical images, \\npredict patient outcom', 'Personalized Learning \\nAI enables personalized learning experiences by adapting to individual student needs and \\nlearning styles. AI-powered platforms provide customized content, feedback, and pacing, \\nenhancing student engagement and outcomes. \\nAdaptive A']\n"
     ]
    }
   ],
   "source": [
    "# Загружаем валидационные данные из JSON-файла\n",
    "with open('data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Извлекаем первый запрос из валидационных данных\n",
    "query = data[3]['question']\n",
    "\n",
    "# Получаем релевантные чанки для каждого размера чанков\n",
    "retrieved_chunks_dict = {size: retrieve_relevant_chunks(query, text_chunks_dict[size], chunk_embeddings_dict[size]) for size in chunk_sizes}\n",
    "\n",
    "# Печатаем найденные чанки для размера чанков 256\n",
    "print(retrieved_chunks_dict[256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация финального ответа\n",
    "Используем LLM для формирования ответа на основе:\n",
    "- Топ-5 наиболее релевантных фрагментов (размер 256 символов)\n",
    "- Строгого системного промта (отвечать только по контексту)\n",
    "- Модели Llama-3 с температурой 0 для детерминированности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI contributes to personalized medicine by analyzing individual patient data, predicting treatment responses, and tailoring interventions. This enables personalized medicine to enhance treatment effectiveness and reduce adverse effects.\n"
     ]
    }
   ],
   "source": [
    "# Определяем системный промт для AI-ассистента\n",
    "system_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n",
    "\n",
    "def generate_response(query, system_prompt, retrieved_chunks, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    Генерирует AI-ответ на основе найденных чанков.\n",
    "\n",
    "    Аргументы:\n",
    "    query (str): Пользовательский запрос.\n",
    "    retrieved_chunks (List[str]): Список найденных текстовых чанков.\n",
    "    model (str): AI-модель.\n",
    "\n",
    "    Возвращает:\n",
    "    str: Сгенерированный AI-ответ.\n",
    "    \"\"\"\n",
    "    # Объединяем найденные чанки в одну строку контекста\n",
    "    context = \"\\n\".join([f\"Context {i+1}:\\n{chunk}\" for i, chunk in enumerate(retrieved_chunks)])\n",
    "    \n",
    "    # Создаём пользовательский промт, объединяя контекст и запрос\n",
    "    user_prompt = f\"{context}\\n\\nQuestion: {query}\"\n",
    "\n",
    "    # Генерируем AI-ответ с использованием указанной модели\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Возвращаем содержимое AI-ответа\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Генерируем AI-ответы для каждого размера чанков\n",
    "ai_responses_dict = {size: generate_response(query, system_prompt, retrieved_chunks_dict[size]) for size in chunk_sizes}\n",
    "\n",
    "# Печатаем ответ для размера чанков 256\n",
    "print(ai_responses_dict[256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка качества ответов\n",
    "Метрики оценки:\n",
    "1. Достоверность (Faithfulness) - соответствие исходным данным\n",
    "2. Релевантность (Relevancy) - соответствие вопросу\n",
    "Оценка проводится автоматически с помощью LLM по строгим критериям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем константы системы оценки\n",
    "SCORE_FULL = 1.0     # Полное совпадение или полностью удовлетворительно\n",
    "SCORE_PARTIAL = 0.5  # Частичное совпадение или частично удовлетворительно\n",
    "SCORE_NONE = 0.0     # Нет совпадения или неудовлетворительно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем шаблоны строгих промтов для оценки\n",
    "FAITHFULNESS_PROMPT_TEMPLATE = \"\"\"\n",
    "Оцените достоверность ответа AI по сравнению с истинным ответом.\n",
    "Пользовательский запрос: {question}\n",
    "Ответ AI: {response}\n",
    "Истинный ответ: {true_answer}\n",
    "\n",
    "Достоверность измеряет, насколько хорошо ответ AI соответствует фактам в истинном ответе, без галлюцинаций.\n",
    "\n",
    "ИНСТРУКЦИИ:\n",
    "- Оценивайте СТРОГО используя только эти значения:\n",
    "    * {full} = Полностью достоверно, нет противоречий с истинным ответом\n",
    "    * {partial} = Частично достоверно, незначительные противоречия\n",
    "    * {none} = Не достоверно, серьёзные противоречия или галлюцинации\n",
    "- Возвращайте ТОЛЬКО числовую оценку ({full}, {partial} или {none}) без объяснений или дополнительного текста.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEVANCY_PROMPT_TEMPLATE = \"\"\"\n",
    "Оцените релевантность ответа AI пользовательскому запросу.\n",
    "Пользовательский запрос: {question}\n",
    "Ответ AI: {response}\n",
    "\n",
    "Релевантность измеряет, насколько хорошо ответ соответствует вопросу пользователя.\n",
    "\n",
    "ИНСТРУКЦИИ:\n",
    "- Оценивайте СТРОГО используя только эти значения:\n",
    "    * {full} = Полностью релевантно, напрямую отвечает на запрос\n",
    "    * {partial} = Частично релевантно, отвечает на некоторые аспекты\n",
    "    * {none} = Не релевантно, не отвечает на запрос\n",
    "- Возвращайте ТОЛЬКО числовую оценку ({full}, {partial} или {none}) без объяснений или дополнительного текста.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness Score (Chunk Size 256): 0.5\n",
      "Relevancy Score (Chunk Size 256): 0.5\n",
      "\n",
      "\n",
      "Faithfulness Score (Chunk Size 128): 0.5\n",
      "Relevancy Score (Chunk Size 128): 0.5\n"
     ]
    }
   ],
   "source": [
    "def evaluate_response(question, response, true_answer):\n",
    "        \"\"\"\n",
    "        Оценивает качество сгенерированного AI-ответа на основе достоверности и релевантности.\n",
    "\n",
    "        Аргументы:\n",
    "        question (str): Оригинальный вопрос пользователя.\n",
    "        response (str): Оцениваемый сгенерированный AI-ответ.\n",
    "        true_answer (str): Правильный ответ, используемый как эталон.\n",
    "\n",
    "        Возвращает:\n",
    "        Tuple[float, float]: Кортеж, содержащий (оценка_достоверности, оценка_релевантности).\n",
    "                                                Каждая оценка одна из: 1.0 (полная), 0.5 (частичная), или 0.0 (нет).\n",
    "        \"\"\"\n",
    "        # Форматируем промты для оценки\n",
    "        faithfulness_prompt = FAITHFULNESS_PROMPT_TEMPLATE.format(\n",
    "                question=question, \n",
    "                response=response, \n",
    "                true_answer=true_answer,\n",
    "                full=SCORE_FULL,\n",
    "                partial=SCORE_PARTIAL,\n",
    "                none=SCORE_NONE\n",
    "        )\n",
    "        \n",
    "        relevancy_prompt = RELEVANCY_PROMPT_TEMPLATE.format(\n",
    "                question=question, \n",
    "                response=response,\n",
    "                full=SCORE_FULL,\n",
    "                partial=SCORE_PARTIAL,\n",
    "                none=SCORE_NONE\n",
    "        )\n",
    "\n",
    "        # Запрашиваем оценку достоверности у модели\n",
    "        faithfulness_response = client.chat.completions.create(\n",
    "               model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an evaluator that returns only numerical scores (1.0, 0.5 or 0.0) with no additional text.\"},\n",
    "                    {\"role\": \"user\", \"content\": faithfulness_prompt}\n",
    "                ]\n",
    "        )\n",
    "        \n",
    "        # Запрашиваем оценку релевантности у модели\n",
    "        relevancy_response = client.chat.completions.create(\n",
    "               model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an evaluator that returns only numerical scores (1.0, 0.5 or 0.0) with no additional text.\"},\n",
    "                    {\"role\": \"user\", \"content\": relevancy_prompt}\n",
    "                ]\n",
    "        )\n",
    "\n",
    "        # Возвращаем оценки достоверности и релевантности\n",
    "        return (float(faithfulness_response.choices[0].message.content), \n",
    "                float(relevancy_response.choices[0].message.content))\n",
    "\n",
    "# Оцениваем ответы для каждого размера чанков\n",
    "scores_dict = {}\n",
    "for size in chunk_sizes:\n",
    "    if size in [256, 128]:  # Оцениваем только эти размеры для примера\n",
    "        faithfulness, relevancy = evaluate_response(query, ai_responses_dict[size], data[3]['ideal_answer'])\n",
    "        scores_dict[size] = (faithfulness, relevancy)\n",
    "        print(f\"Faithfulness Score (Chunk Size {size}): {faithfulness}\")\n",
    "        print(f\"Relevancy Score (Chunk Size {size}): {relevancy}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}