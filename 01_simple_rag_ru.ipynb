{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в простой RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation (RAG) - это гибридный подход, объединяющий информационный поиск с генеративными моделями. Он улучшает производительность языковых моделей за счет включения внешних знаний, что повышает точность и фактическую корректность."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В простой реализации RAG мы выполняем следующие шаги:\n",
    "\n",
    "1. **Загрузка данных**: Загрузка и предварительная обработка текстовых данных.\n",
    "\n",
    "2. **Разбиение на чанки**: Разделение данных на меньшие части для улучшения производительности поиска.\n",
    "\n",
    "3. **Создание эмбеддингов**: Преобразование текстовых чанков в числовые представления с помощью модели эмбеддингов.\n",
    "\n",
    "4. **Семантический поиск**: Поиск релевантных чанков на основе пользовательского запроса.\n",
    "\n",
    "5. **Генерация ответа**: Использование языковой модели для генерации ответа на основе найденного текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот ноутбук реализует простой подход RAG, оценивает ответы модели и исследует различные улучшения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройка окружения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с импорта необходимых библиотек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Извлечение текста из PDF файла"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для реализации RAG нам сначала нужен источник текстовых данных. В данном случае мы извлекаем текст из PDF файла с помощью библиотеки PyMuPDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Извлекает текст из PDF файла.\n",
    "\n",
    "    Аргументы:\n",
    "    pdf_path (str): Путь к PDF файлу.\n",
    "\n",
    "    Возвращает:\n",
    "    str: Извлеченный текст из PDF.\n",
    "    \"\"\"\n",
    "    # Открываем PDF файл\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # Инициализируем пустую строку для хранения текста\n",
    "\n",
    "    # Проходим по каждой странице PDF\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # Получаем страницу\n",
    "        text = page.get_text(\"text\")  # Извлекаем текст со страницы\n",
    "        all_text += text  # Добавляем текст к общему содержимому\n",
    "\n",
    "    return all_text  # Возвращаем извлеченный текст"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбиение извлеченного текста на чанки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После извлечения текста мы разделяем его на меньшие, перекрывающиеся части для повышения точности поиска."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    Разбивает текст на части заданного размера с перекрытием.\n",
    "\n",
    "    Аргументы:\n",
    "    text (str): Текст для разбиения.\n",
    "    n (int): Количество символов в каждом чанке.\n",
    "    overlap (int): Количество перекрывающихся символов между чанками.\n",
    "\n",
    "    Возвращает:\n",
    "    List[str]: Список текстовых чанков.\n",
    "    \"\"\"\n",
    "    chunks = []  # Инициализируем пустой список для чанков\n",
    "    \n",
    "    # Проходим по тексту с шагом (n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Добавляем чанк текста от i до i + n\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # Возвращаем список чанков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройка клиента OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем клиент OpenAI для генерации эмбеддингов и ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем клиент OpenAI с базовым URL и API ключом\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # Получаем API ключ из переменных окружения\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Извлечение и разбиение текста из PDF файла"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы загружаем PDF, извлекаем текст и разбиваем его на чанки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Указываем путь к PDF файлу\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# Извлекаем текст из PDF\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Разбиваем текст на чанки по 1000 символов с перекрытием 200 символов\n",
    "text_chunks = chunk_text(extracted_text, 1000, 200)\n",
    "\n",
    "# Выводим количество созданных чанков\n",
    "print(\"Количество текстовых чанков:\", len(text_chunks))\n",
    "\n",
    "# Выводим первый чанк\n",
    "print(\"\\nПервый текстовый чанк:\")\n",
    "print(text_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание эмбеддингов для текстовых чанков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эмбеддинги преобразуют текст в числовые векторы, что позволяет эффективно выполнять поиск по сходству."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n",
    "    \"\"\"\n",
    "    Создает эмбеддинги для заданного текста с использованием указанной модели.\n",
    "\n",
    "    Аргументы:\n",
    "    text (str): Входной текст для создания эмбеддингов.\n",
    "    model (str): Модель для создания эмбеддингов. По умолчанию \"BAAI/bge-en-icl\".\n",
    "\n",
    "    Возвращает:\n",
    "    dict: Ответ API OpenAI, содержащий эмбеддинги.\n",
    "    \"\"\"\n",
    "    # Создаем эмбеддинги для входного текста с использованием указанной модели\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "\n",
    "    return response  # Возвращаем ответ с эмбеддингами\n",
    "\n",
    "# Создаем эмбеддинги для текстовых чанков\n",
    "response = create_embeddings(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семантический поиск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем косинусную схожесть для поиска наиболее релевантных текстовых чанков по запросу пользователя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Вычисляет косинусную схожесть между двумя векторами.\n",
    "\n",
    "    Аргументы:\n",
    "    vec1 (np.ndarray): Первый вектор.\n",
    "    vec2 (np.ndarray): Второй вектор.\n",
    "\n",
    "    Возвращает:\n",
    "    float: Косинусная схожесть между векторами.\n",
    "    \"\"\"\n",
    "    # Вычисляем скалярное произведение векторов и делим на произведение их норм\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "def semantic_search(query, text_chunks, embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Выполняет семантический поиск по текстовым чанкам с использованием запроса и эмбеддингов.\n",
    "\n",
    "    Аргументы:\n",
    "    query (str): Запрос для семантического поиска.\n",
    "    text_chunks (List[str]): Список текстовых чанков для поиска.\n",
    "    embeddings (List[dict]): Список эмбеддингов для текстовых чанков.\n",
    "    k (int): Количество наиболее релевантных чанков для возврата. По умолчанию 5.\n",
    "\n",
    "    Возвращает:\n",
    "    List[str]: Список k наиболее релевантных текстовых чанков.\n",
    "    \"\"\"\n",
    "    # Создаем эмбеддинг для запроса\n",
    "    query_embedding = create_embeddings(query).data[0].embedding\n",
    "    similarity_scores = []  # Инициализируем список для хранения оценок схожести\n",
    "\n",
    "    # Вычисляем оценки схожести между эмбеддингом запроса и каждым чанком\n",
    "    for i, chunk_embedding in enumerate(embeddings):\n",
    "        similarity_score = cosine_similarity(np.array(query_embedding), np.array(chunk_embedding.embedding))\n",
    "        similarity_scores.append((i, similarity_score))  # Добавляем индекс и оценку\n",
    "\n",
    "    # Сортируем оценки схожести по убыванию\n",
    "    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    # Получаем индексы k наиболее схожих чанков\n",
    "    top_indices = [index for index, _ in similarity_scores[:k]]\n",
    "    # Возвращаем k наиболее релевантных чанков\n",
    "    return [text_chunks[index] for index in top_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выполнение запроса по извлеченным чанкам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем валидационные данные из JSON файла\n",
    "with open('data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Извлекаем первый запрос из валидационных данных\n",
    "query = data[0]['question']\n",
    "\n",
    "# Выполняем семантический поиск для 2 наиболее релевантных чанков\n",
    "top_chunks = semantic_search(query, text_chunks, response.data, k=2)\n",
    "\n",
    "# Выводим запрос\n",
    "print(\"Запрос:\", query)\n",
    "\n",
    "# Выводим 2 наиболее релевантных чанка\n",
    "for i, chunk in enumerate(top_chunks):\n",
    "    print(f\"Контекст {i + 1}:\\n{chunk}\\n=====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация ответа на основе найденных чанков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем системный промт для AI ассистента\n",
    "system_prompt = \"Вы - AI ассистент, который отвечает строго на основе предоставленного контекста. Если ответ не может быть получен непосредственно из контекста, ответьте: 'У меня недостаточно информации для ответа на этот вопрос.'\"\n",
    "\n",
    "def generate_response(system_prompt, user_message, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    Генерирует ответ от AI модели на основе системного промта и пользовательского сообщения.\n",
    "\n",
    "    Аргументы:\n",
    "    system_prompt (str): Системный промт, определяющий поведение AI.\n",
    "    user_message (str): Сообщение или запрос пользователя.\n",
    "    model (str): Модель для генерации ответа. По умолчанию \"meta-llama/Llama-3.2-3B-Instruct\".\n",
    "\n",
    "    Возвращает:\n",
    "    dict: Ответ от AI модели.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Создаем пользовательский промт на основе найденных чанков\n",
    "user_prompt = \"\\n\".join([f\"Контекст {i + 1}:\\n{chunk}\\n=====================================\\n\" for i, chunk in enumerate(top_chunks)])\n",
    "user_prompt = f\"{user_prompt}\\nВопрос: {query}\"\n",
    "\n",
    "# Генерируем ответ AI\n",
    "ai_response = generate_response(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка ответа AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравниваем ответ AI с ожидаемым ответом и присваиваем оценку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем системный промт для системы оценки\n",
    "evaluate_system_prompt = \"Вы - интеллектуальная система оценки, задача которой - анализировать ответы AI ассистента. Если ответ AI очень близок к правильному ответу, присвойте оценку 1. Если ответ неверен или неудовлетворителен по сравнению с правильным ответом, присвойте оценку 0. Если ответ частично соответствует правильному ответу, присвойте оценку 0.5.\"\n",
    "\n",
    "# Создаем промт для оценки, комбинируя запрос, ответ AI, правильный ответ и системный промт\n",
    "evaluation_prompt = f\"Запрос пользователя: {query}\\nОтвет AI:\\n{ai_response.choices[0].message.content}\\nПравильный ответ: {data[0]['ideal_answer']}\\n{evaluate_system_prompt}\"\n",
    "\n",
    "# Генерируем ответ оценки\n",
    "evaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n",
    "\n",
    "# Выводим результат оценки\n",
    "print(evaluation_response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
